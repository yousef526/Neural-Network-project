{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uZSzW2k6roZ"
   },
   "outputs": [],
   "source": [
    "!pip install tflearn\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLXzBhfa5HPb",
    "outputId": "6eef4ea9-224c-4584-f02a-f5e6883471bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "from tensorflow.keras.layers import Input,Dense,Conv2D,Add\n",
    "from tensorflow.keras.layers import SeparableConv2D,ReLU\n",
    "from tensorflow.keras.layers import BatchNormalization,MaxPool2D\n",
    "from tensorflow.keras.layers import GlobalAvgPool2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten,Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7BbdrU8Jih2O"
   },
   "outputs": [],
   "source": [
    "device_list=tf.test.gpu_device_name()\n",
    "device_list\n",
    "if(device_list!='/device:GPU:0'):\n",
    "  raise SystemError('GPU')\n",
    "  print(format(device_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ib20iifH7b5B",
    "outputId": "6450f36f-bd0b-473e-e1f8-e8efd9abc1c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from opendatasets) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from opendatasets) (7.1.2)\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (from opendatasets) (1.5.12)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (7.0.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
      "Installing collected packages: opendatasets\n",
      "Successfully installed opendatasets-0.1.22\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2si_idYED4Br",
    "outputId": "b469d2ba-c23b-467c-b721-d81e9e0bd8cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: yousefalaa86\n",
      "Your Kaggle Key: ··········\n",
      "Downloading nn23-sports-image-classification.zip to ./nn23-sports-image-classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262M/262M [00:07<00:00, 37.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./nn23-sports-image-classification/nn23-sports-image-classification.zip to ./nn23-sports-image-classification\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as dt\n",
    "dt.download(\"https://www.kaggle.com/competitions/nn23-sports-image-classification/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jNWh59TN4rk0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('/content/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vjYncnTS6O61"
   },
   "outputs": [],
   "source": [
    "os.mkdir('/content/images/Basketball')\n",
    "os.mkdir('/content/images/Football')\n",
    "os.mkdir('/content/images/Rowing')\n",
    "os.mkdir('/content/images/Swimming')\n",
    "os.mkdir('/content/images/Tennis')\n",
    "os.mkdir('/content/images/Yoga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPlIHbvr6uC8",
    "outputId": "bd73be5c-e0db-4aff-f1d5-b50692cc0885"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1681/1681 [00:00<00:00, 34321.47it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'nn23-sports-image-classification/Train'\n",
    "import shutil\n",
    "for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "  word_label = img.split('_')[0]\n",
    "  if word_label == 'Basketball':\n",
    "      shutil.move('/content/nn23-sports-image-classification/Train/'+img,'/content/images/Basketball/'+img)\n",
    "  elif word_label == 'Football':\n",
    "      shutil.move('/content/nn23-sports-image-classification/Train/'+img,'/content/images/Football/'+img)\n",
    "  elif word_label == 'Rowing':\n",
    "      shutil.move('/content/nn23-sports-image-classification/Train/'+img,'/content/images/Rowing/'+img)\n",
    "  elif word_label == 'Swimming':\n",
    "      shutil.move('/content/nn23-sports-image-classification/Train/'+img,'/content/images/Swimming/'+img)\n",
    "  elif word_label == 'Tennis':\n",
    "      shutil.move('/content/nn23-sports-image-classification/Train/'+img,'/content/images/Tennis/'+img)\n",
    "  elif word_label == 'Yoga':\n",
    "      shutil.move('/content/nn23-sports-image-classification/Train/'+img,'/content/images/Yoga/'+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXjWhHeZ3vBU",
    "outputId": "f209a3b7-fe6e-4ee7-90cc-66c3c4544b04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1681 files belonging to 6 classes.\n",
      "Using 1345 files for training.\n",
      "Using 336 files for validation.\n"
     ]
    }
   ],
   "source": [
    "data='/content/images/'\n",
    "training,validation=tf.keras.utils.image_dataset_from_directory(\n",
    "    data,\n",
    "    validation_split=0.2,\n",
    "    subset='both',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=16,\n",
    "    image_size=(299, 299),\n",
    "    shuffle=True,\n",
    "    seed=10,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUmUVF3jhq-x",
    "outputId": "46bb037e-1a25-49ff-f22a-b0348cc8cbd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1681 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "all_data=tf.keras.utils.image_dataset_from_directory(\n",
    "    data,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(299, 299),\n",
    "    shuffle=True,\n",
    "    seed=10,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2MvhyGjBWb0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVU_4iHeI93B"
   },
   "outputs": [],
   "source": [
    "# our xception model hand made\n",
    "\n",
    "\n",
    "#import necessary libraries\n",
    "\n",
    "\n",
    "# creating the Conv-Batch Norm block\n",
    "\n",
    "def conv(x, filters, kernel_size, strides=1):\n",
    "    \n",
    "    x = Conv2D(filters=filters, \n",
    "               kernel_size = kernel_size,\n",
    "               strides=strides, \n",
    "               padding = 'same', \n",
    "               use_bias = False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# creating separableConv-Batch Norm block to speed up the convolution \n",
    "def sep_conv(x, filters, kernel_size, strides=1):\n",
    "    #Depthwise Separable Convolutions\n",
    "    x = SeparableConv2D(filters=filters, \n",
    "                        kernel_size = kernel_size, \n",
    "                        strides=strides, \n",
    "                        padding = 'same', \n",
    "                        use_bias = False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "# entry flow\n",
    "\n",
    "def entry_flow(x):\n",
    "    \n",
    "    x = conv(x, filters =32, kernel_size =3, strides=2)\n",
    "    x = ReLU()(x) #activation function\n",
    "    x = conv(x, filters =64, kernel_size =3, strides=1)\n",
    "    tensor = ReLU()(x)\n",
    "    \n",
    "    x = sep_conv(tensor, filters = 128, kernel_size =3)\n",
    "    x = ReLU()(x)\n",
    "    x = sep_conv(x, filters = 128, kernel_size =3)\n",
    "    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)\n",
    "    \n",
    "    #skip connection\n",
    "    tensor = conv(tensor, filters=128, kernel_size = 1,strides=2)\n",
    "    #using add to merge the two tensors\n",
    "    x = Add()([tensor,x])\n",
    "    \n",
    "    x = ReLU()(x)\n",
    "    x = sep_conv(x, filters =256, kernel_size=3)\n",
    "    x = ReLU()(x)\n",
    "    x = sep_conv(x, filters =256, kernel_size=3)\n",
    "    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)\n",
    "    \n",
    "    tensor = conv(tensor, filters=256, kernel_size = 1,strides=2)\n",
    "    x = Add()([tensor,x])\n",
    "    \n",
    "    x = ReLU()(x)\n",
    "    x = sep_conv(x, filters =728, kernel_size=3)\n",
    "    x = ReLU()(x)\n",
    "    x = sep_conv(x, filters =728, kernel_size=3)\n",
    "    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)\n",
    "    \n",
    "    tensor = conv(tensor, filters=728, kernel_size = 1,strides=2)\n",
    "    x = Add()([tensor,x])\n",
    "    return x\n",
    "    # img size after entry flow is: 19*19*728\n",
    "# middle flow\n",
    "\n",
    "def middle_flow(tensor):\n",
    "    \n",
    "    for _ in range(8):\n",
    "        x = ReLU()(tensor)\n",
    "        x = sep_conv(x, filters = 728, kernel_size = 3)\n",
    "        x = ReLU()(x)\n",
    "        x = sep_conv(x, filters = 728, kernel_size = 3)\n",
    "        x = ReLU()(x)\n",
    "        x = sep_conv(x, filters = 728, kernel_size = 3)\n",
    "        x = ReLU()(x)\n",
    "        tensor = Add()([tensor,x])\n",
    "\n",
    "    return tensor\n",
    "    # img size after entry flow is: 19*19*728\n",
    "# exit flow\n",
    "\n",
    "def exit_flow(tensor):\n",
    "    \n",
    "    x = ReLU()(tensor)\n",
    "    x = sep_conv(x, filters = 728,  kernel_size=3)\n",
    "    x = ReLU()(x)\n",
    "    x = sep_conv(x, filters = 1024,  kernel_size=3)\n",
    "    x = MaxPool2D(pool_size = 3, strides = 2, padding ='same')(x)\n",
    "    \n",
    "    tensor = conv_bn(tensor, filters =1024, kernel_size=1, strides =2)\n",
    "    x = Add()([tensor,x])\n",
    "    \n",
    "    x = sep_conv(x, filters = 1536,  kernel_size=3)\n",
    "    x = ReLU()(x)\n",
    "    x = sep_conv(x, filters = 2048,  kernel_size=3)\n",
    "    x = GlobalAvgPool2D()(x)\n",
    "    \n",
    "    x = Dense(units = 1000, activation = 'softmax')(x)\n",
    "    \n",
    "    return x\n",
    "# model code\n",
    "\n",
    "input = Input(shape = (299,299,3))\n",
    "x = entry_flow(input)\n",
    "x = middle_flow(x)\n",
    "output = exit_flow(x)\n",
    "\n",
    "newmodel = Model (inputs=input, outputs=output)\n",
    "newmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nlWf9EAJYFC",
    "outputId": "aecd28a7-69f3-4236-ccc0-e33fac7311e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5d45580> and <keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7f78b5d7c700>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5cdbac0> and <keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7f78b5caab80>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5c6edc0> and <keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7f78b5bc2910>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5ad73a0> and <keras.layers.activation.relu.ReLU object at 0x7f78b5bbcd00>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5b20790> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5ad73a0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5a8a8e0> and <keras.layers.activation.relu.ReLU object at 0x7f78b5b37520>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5aec1f0> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5a8a8e0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5a405b0> and <keras.layers.activation.relu.ReLU object at 0x7f78b5af93a0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5a8a820> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5a405b0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5a11ee0> and <keras.layers.merging.add.Add object at 0x7f78b5ab6280>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5a405e0> and <keras.layers.activation.relu.ReLU object at 0x7f78b5a75640>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5a2cdc0> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5a405e0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5a7c250> and <keras.layers.activation.relu.ReLU object at 0x7f78b5a47c70>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b59eb3d0> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5a7c250>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5a2c9d0> and <keras.layers.activation.relu.ReLU object at 0x7f78b5a1c2b0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b59aec10> and <keras.layers.activation.relu.ReLU object at 0x7f78b5db7460>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b59eb100> and <keras.layers.merging.add.Add object at 0x7f78b59dadc0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5a33220> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b59aec10>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b59ae760> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b59eb100>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b597df40> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5a33220>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b594f0a0> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b59ae760>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b58c9fa0> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b594f0a0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5929730> and <keras.layers.activation.relu.ReLU object at 0x7f78b596ad90>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b58e6af0> and <keras.layers.activation.relu.ReLU object at 0x7f78b599ac70>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b58c9fd0> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b58c9fa0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b589dd60> and <keras.layers.activation.relu.ReLU object at 0x7f78b5913460>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b58d4c10> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b58e6af0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b586a280> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b589dd60>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b589d910> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b58d4c10>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5809940> and <keras.layers.merging.add.Add object at 0x7f78b5892640>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5858640> and <keras.layers.activation.relu.ReLU object at 0x7f78b5861fd0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b27b98e0> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b589d910>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b58098e0> and <keras.layers.activation.relu.ReLU object at 0x7f78b58b5ee0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b27f3760> and <keras.layers.activation.relu.ReLU object at 0x7f78b5877580>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b27b95e0> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b27b98e0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f7936cb2af0> and <keras.layers.activation.relu.ReLU object at 0x7f78b5822ee0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f7936cb2b50> and <keras.layers.merging.add.Add object at 0x7f78b58352e0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f792005c280> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b27f3760>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f79200641c0> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b27b95e0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78cc321190> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f7936cb2b50>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78cc2d1df0> and <keras.layers.activation.relu.ReLU object at 0x7f7936a57580>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78cc1dc460> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f79200641c0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5f889d0> and <keras.layers.activation.relu.ReLU object at 0x7f78cc4f3f40>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5f9e4f0> and <keras.layers.activation.relu.ReLU object at 0x7f78cc2df0d0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b27afb80> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78cc321190>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b2688760> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78cc1dc460>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b2688970> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b5f889d0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b26ac4c0> and <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b5f9e4f0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.conv2d.Conv2D object at 0x7f78b265ba90> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b27afb80>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b26ac7c0> and <keras.layers.activation.relu.ReLU object at 0x7f78b26ebeb0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b264b520> and <keras.layers.activation.relu.ReLU object at 0x7f78b5c23df0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b2618430> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b2688970>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b2603610> and <keras.layers.activation.relu.ReLU object at 0x7f78b269c1f0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f78b25d11f0> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f78b265ba90>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b2633cd0> and <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f78b26ac7c0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.core.dense.Dense object at 0x7f78b25ea640> and <keras.layers.merging.add.Add object at 0x7f78b260a340>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85 steps, validate on 21 steps\n",
      "Epoch 1/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.9198 - acc: 0.6394"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_acc improved from -inf to 0.13095, saving model to /content/best_weights\n",
      "85/85 [==============================] - 79s 759ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.9198 - acc: 0.6394 - val_loss: 5.6319 - val_acc: 0.1310\n",
      "Epoch 2/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.3675 - acc: 0.8937\n",
      "Epoch 2: val_acc improved from 0.13095 to 0.32143, saving model to /content/best_weights\n",
      "85/85 [==============================] - 53s 620ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.3675 - acc: 0.8937 - val_loss: 5.5695 - val_acc: 0.3214\n",
      "Epoch 3/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0936 - acc: 0.9792\n",
      "Epoch 3: val_acc did not improve from 0.32143\n",
      "85/85 [==============================] - 54s 627ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0936 - acc: 0.9792 - val_loss: 5.0706 - val_acc: 0.3095\n",
      "Epoch 4/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0439 - acc: 0.9896\n",
      "Epoch 4: val_acc improved from 0.32143 to 0.72619, saving model to /content/best_weights\n",
      "85/85 [==============================] - 55s 639ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0439 - acc: 0.9896 - val_loss: 2.3329 - val_acc: 0.7262\n",
      "Epoch 5/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0686 - acc: 0.9814\n",
      "Epoch 5: val_acc improved from 0.72619 to 0.83036, saving model to /content/best_weights\n",
      "85/85 [==============================] - 55s 642ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0686 - acc: 0.9814 - val_loss: 1.0469 - val_acc: 0.8304\n",
      "Epoch 6/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0872 - acc: 0.9770\n",
      "Epoch 6: val_acc did not improve from 0.83036\n",
      "85/85 [==============================] - 54s 629ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0872 - acc: 0.9770 - val_loss: 2.4483 - val_acc: 0.7024\n",
      "Epoch 7/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0732 - acc: 0.9784\n",
      "Epoch 7: val_acc improved from 0.83036 to 0.87202, saving model to /content/best_weights\n",
      "85/85 [==============================] - 55s 639ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0732 - acc: 0.9784 - val_loss: 0.9216 - val_acc: 0.8720\n",
      "Epoch 8/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0325 - acc: 0.9941\n",
      "Epoch 8: val_acc did not improve from 0.87202\n",
      "85/85 [==============================] - 54s 636ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0325 - acc: 0.9941 - val_loss: 0.8228 - val_acc: 0.8482\n",
      "Epoch 9/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0195 - acc: 0.9948\n",
      "Epoch 9: val_acc did not improve from 0.87202\n",
      "85/85 [==============================] - 54s 632ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0195 - acc: 0.9948 - val_loss: 0.6095 - val_acc: 0.8690\n",
      "Epoch 10/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0069 - acc: 0.9978\n",
      "Epoch 10: val_acc did not improve from 0.87202\n",
      "85/85 [==============================] - 54s 637ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0069 - acc: 0.9978 - val_loss: 0.6673 - val_acc: 0.8690\n",
      "Epoch 11/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0327 - acc: 0.9903\n",
      "Epoch 11: val_acc did not improve from 0.87202\n",
      "85/85 [==============================] - 54s 628ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0327 - acc: 0.9903 - val_loss: 1.8302 - val_acc: 0.7798\n",
      "Epoch 12/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.1221 - acc: 0.9576\n",
      "Epoch 12: val_acc did not improve from 0.87202\n",
      "85/85 [==============================] - 54s 629ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.1221 - acc: 0.9576 - val_loss: 1.1811 - val_acc: 0.8452\n",
      "Epoch 13/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0794 - acc: 0.9747\n",
      "Epoch 13: val_acc did not improve from 0.87202\n",
      "85/85 [==============================] - 54s 629ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0794 - acc: 0.9747 - val_loss: 1.4971 - val_acc: 0.7917\n",
      "Epoch 14/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0443 - acc: 0.9814\n",
      "Epoch 14: val_acc did not improve from 0.87202\n",
      "85/85 [==============================] - 54s 630ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0443 - acc: 0.9814 - val_loss: 1.2909 - val_acc: 0.8333\n",
      "Epoch 15/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0128 - acc: 0.9955\n",
      "Epoch 15: val_acc improved from 0.87202 to 0.87500, saving model to /content/best_weights\n",
      "85/85 [==============================] - 55s 639ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0128 - acc: 0.9955 - val_loss: 1.0417 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0110 - acc: 0.9970\n",
      "Epoch 16: val_acc improved from 0.87500 to 0.87798, saving model to /content/best_weights\n",
      "85/85 [==============================] - 55s 640ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0110 - acc: 0.9970 - val_loss: 0.8064 - val_acc: 0.8780\n",
      "Epoch 17/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 0.0018 - acc: 1.0000    \n",
      "Epoch 17: val_acc improved from 0.87798 to 0.89286, saving model to /content/best_weights\n",
      "85/85 [==============================] - 55s 640ms/step - batch: 42.0000 - size: 1.0000 - loss: 0.0018 - acc: 1.0000 - val_loss: 0.7823 - val_acc: 0.8929\n",
      "Epoch 18/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 8.7257e-04 - acc: 1.0000\n",
      "Epoch 18: val_acc improved from 0.89286 to 0.89583, saving model to /content/best_weights\n",
      "85/85 [==============================] - 55s 644ms/step - batch: 42.0000 - size: 1.0000 - loss: 8.7257e-04 - acc: 1.0000 - val_loss: 0.8300 - val_acc: 0.8958\n",
      "Epoch 19/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 6.6914e-04 - acc: 1.0000\n",
      "Epoch 19: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 634ms/step - batch: 42.0000 - size: 1.0000 - loss: 6.6914e-04 - acc: 1.0000 - val_loss: 0.8598 - val_acc: 0.8929\n",
      "Epoch 20/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 5.5762e-04 - acc: 1.0000\n",
      "Epoch 20: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 626ms/step - batch: 42.0000 - size: 1.0000 - loss: 5.5762e-04 - acc: 1.0000 - val_loss: 0.8784 - val_acc: 0.8899\n",
      "Epoch 21/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 4.7625e-04 - acc: 1.0000\n",
      "Epoch 21: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 630ms/step - batch: 42.0000 - size: 1.0000 - loss: 4.7625e-04 - acc: 1.0000 - val_loss: 0.8911 - val_acc: 0.8899\n",
      "Epoch 22/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 4.1383e-04 - acc: 1.0000\n",
      "Epoch 22: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 629ms/step - batch: 42.0000 - size: 1.0000 - loss: 4.1383e-04 - acc: 1.0000 - val_loss: 0.9009 - val_acc: 0.8899\n",
      "Epoch 23/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 3.6427e-04 - acc: 1.0000\n",
      "Epoch 23: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 629ms/step - batch: 42.0000 - size: 1.0000 - loss: 3.6427e-04 - acc: 1.0000 - val_loss: 0.9090 - val_acc: 0.8899\n",
      "Epoch 24/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 3.2391e-04 - acc: 1.0000\n",
      "Epoch 24: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 630ms/step - batch: 42.0000 - size: 1.0000 - loss: 3.2391e-04 - acc: 1.0000 - val_loss: 0.9161 - val_acc: 0.8899\n",
      "Epoch 25/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 2.9042e-04 - acc: 1.0000\n",
      "Epoch 25: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 628ms/step - batch: 42.0000 - size: 1.0000 - loss: 2.9042e-04 - acc: 1.0000 - val_loss: 0.9226 - val_acc: 0.8899\n",
      "Epoch 26/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 2.6217e-04 - acc: 1.0000\n",
      "Epoch 26: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 55s 639ms/step - batch: 42.0000 - size: 1.0000 - loss: 2.6217e-04 - acc: 1.0000 - val_loss: 0.9286 - val_acc: 0.8869\n",
      "Epoch 27/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 2.3803e-04 - acc: 1.0000\n",
      "Epoch 27: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 53s 621ms/step - batch: 42.0000 - size: 1.0000 - loss: 2.3803e-04 - acc: 1.0000 - val_loss: 0.9342 - val_acc: 0.8899\n",
      "Epoch 28/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 2.1718e-04 - acc: 1.0000\n",
      "Epoch 28: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 633ms/step - batch: 42.0000 - size: 1.0000 - loss: 2.1718e-04 - acc: 1.0000 - val_loss: 0.9396 - val_acc: 0.8899\n",
      "Epoch 29/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.9900e-04 - acc: 1.0000\n",
      "Epoch 29: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 635ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.9900e-04 - acc: 1.0000 - val_loss: 0.9447 - val_acc: 0.8899\n",
      "Epoch 30/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.8302e-04 - acc: 1.0000\n",
      "Epoch 30: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 627ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.8302e-04 - acc: 1.0000 - val_loss: 0.9496 - val_acc: 0.8899\n",
      "Epoch 31/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.6888e-04 - acc: 1.0000\n",
      "Epoch 31: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 628ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.6888e-04 - acc: 1.0000 - val_loss: 0.9543 - val_acc: 0.8899\n",
      "Epoch 32/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.5628e-04 - acc: 1.0000\n",
      "Epoch 32: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 632ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.5628e-04 - acc: 1.0000 - val_loss: 0.9588 - val_acc: 0.8899\n",
      "Epoch 33/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.4500e-04 - acc: 1.0000\n",
      "Epoch 33: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 55s 639ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.4500e-04 - acc: 1.0000 - val_loss: 0.9632 - val_acc: 0.8899\n",
      "Epoch 34/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.3485e-04 - acc: 1.0000\n",
      "Epoch 34: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 53s 622ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.3485e-04 - acc: 1.0000 - val_loss: 0.9674 - val_acc: 0.8899\n",
      "Epoch 35/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.2568e-04 - acc: 1.0000\n",
      "Epoch 35: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 634ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.2568e-04 - acc: 1.0000 - val_loss: 0.9715 - val_acc: 0.8899\n",
      "Epoch 36/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.1736e-04 - acc: 1.0000\n",
      "Epoch 36: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 636ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.1736e-04 - acc: 1.0000 - val_loss: 0.9755 - val_acc: 0.8899\n",
      "Epoch 37/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.0978e-04 - acc: 1.0000\n",
      "Epoch 37: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 627ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.0978e-04 - acc: 1.0000 - val_loss: 0.9794 - val_acc: 0.8899\n",
      "Epoch 38/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 1.0286e-04 - acc: 1.0000\n",
      "Epoch 38: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 627ms/step - batch: 42.0000 - size: 1.0000 - loss: 1.0286e-04 - acc: 1.0000 - val_loss: 0.9832 - val_acc: 0.8899\n",
      "Epoch 39/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 9.6513e-05 - acc: 1.0000\n",
      "Epoch 39: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 53s 626ms/step - batch: 42.0000 - size: 1.0000 - loss: 9.6513e-05 - acc: 1.0000 - val_loss: 0.9869 - val_acc: 0.8899\n",
      "Epoch 40/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 9.0682e-05 - acc: 1.0000\n",
      "Epoch 40: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 627ms/step - batch: 42.0000 - size: 1.0000 - loss: 9.0682e-05 - acc: 1.0000 - val_loss: 0.9905 - val_acc: 0.8899\n",
      "Epoch 41/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 8.5317e-05 - acc: 1.0000\n",
      "Epoch 41: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 627ms/step - batch: 42.0000 - size: 1.0000 - loss: 8.5317e-05 - acc: 1.0000 - val_loss: 0.9940 - val_acc: 0.8899\n",
      "Epoch 42/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 8.0356e-05 - acc: 1.0000\n",
      "Epoch 42: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 629ms/step - batch: 42.0000 - size: 1.0000 - loss: 8.0356e-05 - acc: 1.0000 - val_loss: 0.9975 - val_acc: 0.8869\n",
      "Epoch 43/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 7.5760e-05 - acc: 1.0000\n",
      "Epoch 43: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 632ms/step - batch: 42.0000 - size: 1.0000 - loss: 7.5760e-05 - acc: 1.0000 - val_loss: 1.0009 - val_acc: 0.8869\n",
      "Epoch 44/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 7.1503e-05 - acc: 1.0000\n",
      "Epoch 44: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 628ms/step - batch: 42.0000 - size: 1.0000 - loss: 7.1503e-05 - acc: 1.0000 - val_loss: 1.0042 - val_acc: 0.8869\n",
      "Epoch 45/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 6.7551e-05 - acc: 1.0000\n",
      "Epoch 45: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 53s 624ms/step - batch: 42.0000 - size: 1.0000 - loss: 6.7551e-05 - acc: 1.0000 - val_loss: 1.0075 - val_acc: 0.8869\n",
      "Epoch 46/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 6.3867e-05 - acc: 1.0000\n",
      "Epoch 46: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 630ms/step - batch: 42.0000 - size: 1.0000 - loss: 6.3867e-05 - acc: 1.0000 - val_loss: 1.0107 - val_acc: 0.8869\n",
      "Epoch 47/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 6.0437e-05 - acc: 1.0000\n",
      "Epoch 47: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 53s 626ms/step - batch: 42.0000 - size: 1.0000 - loss: 6.0437e-05 - acc: 1.0000 - val_loss: 1.0139 - val_acc: 0.8869\n",
      "Epoch 48/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 5.7231e-05 - acc: 1.0000\n",
      "Epoch 48: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 632ms/step - batch: 42.0000 - size: 1.0000 - loss: 5.7231e-05 - acc: 1.0000 - val_loss: 1.0170 - val_acc: 0.8869\n",
      "Epoch 49/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 5.4239e-05 - acc: 1.0000\n",
      "Epoch 49: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 53s 625ms/step - batch: 42.0000 - size: 1.0000 - loss: 5.4239e-05 - acc: 1.0000 - val_loss: 1.0201 - val_acc: 0.8869\n",
      "Epoch 50/50\n",
      "85/85 [==============================] - ETA: 0s - batch: 42.0000 - size: 1.0000 - loss: 5.1431e-05 - acc: 1.0000\n",
      "Epoch 50: val_acc did not improve from 0.89583\n",
      "85/85 [==============================] - 54s 634ms/step - batch: 42.0000 - size: 1.0000 - loss: 5.1431e-05 - acc: 1.0000 - val_loss: 1.0232 - val_acc: 0.8869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7921077b80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/content/best_weights',\n",
    "    verbose=1,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "#path='/content/drive/MyDrive/weights/imagenet_weigths'\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "newmodel.compile(loss='sparse_categorical_crossentropy' ,optimizer=opt, metrics=['accuracy'])\n",
    "newmodel.load_weights('/content/imagenet_weights')\n",
    "newmodel.fit(training,epochs=50,validation_data=validation,callbacks=[model_checkpoint_callback])\n",
    "#final_model=model.load('/content/best_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrtS2NMNgb77",
    "outputId": "492832c5-4ff0-4470-e1d0-c4c43a5c94b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53 steps\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 68s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 0.3347 - acc: 0.9042\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 60s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 0.1453 - acc: 0.9548\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 0.0473 - acc: 0.9798\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 0.0297 - acc: 0.9911\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 0.0075 - acc: 0.9976\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 8.1854e-04 - acc: 1.0000\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.8320e-04 - acc: 1.0000\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.3356e-04 - acc: 1.0000\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.0598e-04 - acc: 1.0000\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 8.7966e-05 - acc: 1.0000\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 7.5066e-05 - acc: 1.0000\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 6.5295e-05 - acc: 1.0000\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 5.7615e-05 - acc: 1.0000\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 5.1406e-05 - acc: 1.0000\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 4.6279e-05 - acc: 1.0000\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 4.1968e-05 - acc: 1.0000\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 3.8292e-05 - acc: 1.0000\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 3.5119e-05 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 3.2359e-05 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 2.9931e-05 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 2.7784e-05 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 2.5868e-05 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 2.4149e-05 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 2.2601e-05 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 2.1199e-05 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.9922e-05 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.8755e-05 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.7687e-05 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.6705e-05 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.5799e-05 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.4962e-05 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.4188e-05 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.3466e-05 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.2793e-05 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.2167e-05 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.1581e-05 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.1030e-05 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.0516e-05 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 1.0031e-05 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 9.5771e-06 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 9.1472e-06 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 8.7402e-06 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 8.3571e-06 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 7.9941e-06 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 7.6489e-06 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 7.3227e-06 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 7.0118e-06 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 62s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 6.7178e-06 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 6.4353e-06 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 61s 1s/step - batch: 26.0000 - size: 1.0000 - loss: 6.1671e-06 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79ab78f070>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel.load_weights('/content/best_weights')\n",
    "newmodel.fit(all_data,epochs=50)#,callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7ik9tJte63pZ"
   },
   "outputs": [],
   "source": [
    "newmodel.save('content/location/best_weight_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jTW8MSUGvqco"
   },
   "outputs": [],
   "source": [
    "TEST_DIR = '/content/nn23-sports-image-classification/Test'\n",
    "\n",
    "def load_images_test_from_folder(folder):\n",
    "    images = []\n",
    "    images_names = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_data = cv2.resize(img, (299,299))\n",
    "        if img is not None:\n",
    "            images.append([np.array(img_data)])\n",
    "            images_names.append(filename)\n",
    "    return images , images_names\n",
    "\n",
    "\n",
    "\n",
    "test,images_names = load_images_test_from_folder(TEST_DIR)\n",
    "X_test = np.array([i for i in test]).reshape((-1, 299, 299, 3))\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "predictions = newmodel.predict(X_test)\n",
    "pred = []\n",
    "\n",
    "for prediction in predictions:\n",
    "    max_val = np.argmax(prediction)\n",
    "    pred.append(max_val)\n",
    "\n",
    "import csv\n",
    "headers = [\"image_name\", \"label\"]\n",
    "OutPut_list = []\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    x = [images_names[i],pred[i]]\n",
    "    OutPut_list.append(x)\n",
    "\n",
    "with open(\"Sports1.csv\", \"w\") as Sport:\n",
    "    student = csv.writer(Sport)\n",
    "    student.writerow(headers)\n",
    "    student.writerows (OutPut_list)\n",
    "\n",
    "newmodel.save_weights('/content/xception_weights')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
